{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>fips</th>\n",
                            "      <th>TOT_POP</th>\n",
                            "      <th>0-9</th>\n",
                            "      <th>0-9 y/o % of total pop</th>\n",
                            "      <th>19-Oct</th>\n",
                            "      <th>10-19 y/o % of total pop</th>\n",
                            "      <th>20-29</th>\n",
                            "      <th>20-29 y/o % of total pop</th>\n",
                            "      <th>30-39</th>\n",
                            "      <th>30-39 y/o % of total pop</th>\n",
                            "      <th>...</th>\n",
                            "      <th>COPD_number</th>\n",
                            "      <th>diabetes_prevalence</th>\n",
                            "      <th>diabetes_Lower 95% CI</th>\n",
                            "      <th>diabetes_Upper 95% CI</th>\n",
                            "      <th>diabetes_number</th>\n",
                            "      <th>CKD_prevalence</th>\n",
                            "      <th>CKD_Lower 95% CI</th>\n",
                            "      <th>CKD_Upper 95% CI</th>\n",
                            "      <th>CKD_number</th>\n",
                            "      <th>Urban_rural_code</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>1001</td>\n",
                            "      <td>55601</td>\n",
                            "      <td>6787</td>\n",
                            "      <td>12.206615</td>\n",
                            "      <td>7637</td>\n",
                            "      <td>13.735364</td>\n",
                            "      <td>6878</td>\n",
                            "      <td>12.370281</td>\n",
                            "      <td>7089</td>\n",
                            "      <td>12.749771</td>\n",
                            "      <td>...</td>\n",
                            "      <td>3644</td>\n",
                            "      <td>12.9</td>\n",
                            "      <td>11.9</td>\n",
                            "      <td>13.8</td>\n",
                            "      <td>5462</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>2.9</td>\n",
                            "      <td>3.3</td>\n",
                            "      <td>1326</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>1003</td>\n",
                            "      <td>218022</td>\n",
                            "      <td>24757</td>\n",
                            "      <td>11.355276</td>\n",
                            "      <td>26913</td>\n",
                            "      <td>12.344167</td>\n",
                            "      <td>23579</td>\n",
                            "      <td>10.814964</td>\n",
                            "      <td>25213</td>\n",
                            "      <td>11.564429</td>\n",
                            "      <td>...</td>\n",
                            "      <td>14692</td>\n",
                            "      <td>12.0</td>\n",
                            "      <td>11.0</td>\n",
                            "      <td>13.1</td>\n",
                            "      <td>20520</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>3.5</td>\n",
                            "      <td>5479</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>1005</td>\n",
                            "      <td>24881</td>\n",
                            "      <td>2732</td>\n",
                            "      <td>10.980266</td>\n",
                            "      <td>2960</td>\n",
                            "      <td>11.896628</td>\n",
                            "      <td>3268</td>\n",
                            "      <td>13.134520</td>\n",
                            "      <td>3201</td>\n",
                            "      <td>12.865239</td>\n",
                            "      <td>...</td>\n",
                            "      <td>2373</td>\n",
                            "      <td>19.7</td>\n",
                            "      <td>18.6</td>\n",
                            "      <td>20.6</td>\n",
                            "      <td>3870</td>\n",
                            "      <td>4.5</td>\n",
                            "      <td>4.2</td>\n",
                            "      <td>4.8</td>\n",
                            "      <td>887</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>1007</td>\n",
                            "      <td>22400</td>\n",
                            "      <td>2456</td>\n",
                            "      <td>10.964286</td>\n",
                            "      <td>2596</td>\n",
                            "      <td>11.589286</td>\n",
                            "      <td>3029</td>\n",
                            "      <td>13.522321</td>\n",
                            "      <td>3113</td>\n",
                            "      <td>13.897321</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1789</td>\n",
                            "      <td>14.1</td>\n",
                            "      <td>13.2</td>\n",
                            "      <td>14.9</td>\n",
                            "      <td>2511</td>\n",
                            "      <td>3.3</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>3.6</td>\n",
                            "      <td>595</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>1009</td>\n",
                            "      <td>57840</td>\n",
                            "      <td>7095</td>\n",
                            "      <td>12.266598</td>\n",
                            "      <td>7570</td>\n",
                            "      <td>13.087828</td>\n",
                            "      <td>6742</td>\n",
                            "      <td>11.656293</td>\n",
                            "      <td>6884</td>\n",
                            "      <td>11.901798</td>\n",
                            "      <td>...</td>\n",
                            "      <td>4661</td>\n",
                            "      <td>13.5</td>\n",
                            "      <td>12.6</td>\n",
                            "      <td>14.5</td>\n",
                            "      <td>6017</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.7</td>\n",
                            "      <td>1507</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3135</th>\n",
                            "      <td>56037</td>\n",
                            "      <td>43051</td>\n",
                            "      <td>6104</td>\n",
                            "      <td>14.178532</td>\n",
                            "      <td>6326</td>\n",
                            "      <td>14.694200</td>\n",
                            "      <td>5359</td>\n",
                            "      <td>12.448027</td>\n",
                            "      <td>6577</td>\n",
                            "      <td>15.277229</td>\n",
                            "      <td>...</td>\n",
                            "      <td>2098</td>\n",
                            "      <td>8.9</td>\n",
                            "      <td>8.3</td>\n",
                            "      <td>9.6</td>\n",
                            "      <td>2834</td>\n",
                            "      <td>2.6</td>\n",
                            "      <td>2.4</td>\n",
                            "      <td>2.8</td>\n",
                            "      <td>821</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3136</th>\n",
                            "      <td>56039</td>\n",
                            "      <td>23081</td>\n",
                            "      <td>2384</td>\n",
                            "      <td>10.328842</td>\n",
                            "      <td>2185</td>\n",
                            "      <td>9.466661</td>\n",
                            "      <td>2967</td>\n",
                            "      <td>12.854729</td>\n",
                            "      <td>4093</td>\n",
                            "      <td>17.733200</td>\n",
                            "      <td>...</td>\n",
                            "      <td>928</td>\n",
                            "      <td>7.2</td>\n",
                            "      <td>6.5</td>\n",
                            "      <td>8.0</td>\n",
                            "      <td>1360</td>\n",
                            "      <td>2.4</td>\n",
                            "      <td>2.2</td>\n",
                            "      <td>2.6</td>\n",
                            "      <td>447</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3137</th>\n",
                            "      <td>56041</td>\n",
                            "      <td>20299</td>\n",
                            "      <td>3121</td>\n",
                            "      <td>15.375142</td>\n",
                            "      <td>3205</td>\n",
                            "      <td>15.788955</td>\n",
                            "      <td>2153</td>\n",
                            "      <td>10.606434</td>\n",
                            "      <td>2702</td>\n",
                            "      <td>13.311001</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1163</td>\n",
                            "      <td>10.4</td>\n",
                            "      <td>9.5</td>\n",
                            "      <td>11.2</td>\n",
                            "      <td>1500</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>2.8</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>430</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3138</th>\n",
                            "      <td>56043</td>\n",
                            "      <td>7885</td>\n",
                            "      <td>858</td>\n",
                            "      <td>10.881420</td>\n",
                            "      <td>1113</td>\n",
                            "      <td>14.115409</td>\n",
                            "      <td>715</td>\n",
                            "      <td>9.067850</td>\n",
                            "      <td>903</td>\n",
                            "      <td>11.452124</td>\n",
                            "      <td>...</td>\n",
                            "      <td>506</td>\n",
                            "      <td>11.3</td>\n",
                            "      <td>10.3</td>\n",
                            "      <td>12.1</td>\n",
                            "      <td>686</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.7</td>\n",
                            "      <td>207</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3139</th>\n",
                            "      <td>56045</td>\n",
                            "      <td>6967</td>\n",
                            "      <td>780</td>\n",
                            "      <td>11.195637</td>\n",
                            "      <td>779</td>\n",
                            "      <td>11.181283</td>\n",
                            "      <td>681</td>\n",
                            "      <td>9.774652</td>\n",
                            "      <td>906</td>\n",
                            "      <td>13.004162</td>\n",
                            "      <td>...</td>\n",
                            "      <td>480</td>\n",
                            "      <td>11.7</td>\n",
                            "      <td>10.7</td>\n",
                            "      <td>12.7</td>\n",
                            "      <td>644</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>3.6</td>\n",
                            "      <td>185</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>3140 rows × 108 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "       fips  TOT_POP    0-9  0-9 y/o % of total pop  19-Oct  \\\n",
                            "0      1001    55601   6787               12.206615    7637   \n",
                            "1      1003   218022  24757               11.355276   26913   \n",
                            "2      1005    24881   2732               10.980266    2960   \n",
                            "3      1007    22400   2456               10.964286    2596   \n",
                            "4      1009    57840   7095               12.266598    7570   \n",
                            "...     ...      ...    ...                     ...     ...   \n",
                            "3135  56037    43051   6104               14.178532    6326   \n",
                            "3136  56039    23081   2384               10.328842    2185   \n",
                            "3137  56041    20299   3121               15.375142    3205   \n",
                            "3138  56043     7885    858               10.881420    1113   \n",
                            "3139  56045     6967    780               11.195637     779   \n",
                            "\n",
                            "      10-19 y/o % of total pop  20-29  20-29 y/o % of total pop  30-39  \\\n",
                            "0                    13.735364   6878                 12.370281   7089   \n",
                            "1                    12.344167  23579                 10.814964  25213   \n",
                            "2                    11.896628   3268                 13.134520   3201   \n",
                            "3                    11.589286   3029                 13.522321   3113   \n",
                            "4                    13.087828   6742                 11.656293   6884   \n",
                            "...                        ...    ...                       ...    ...   \n",
                            "3135                 14.694200   5359                 12.448027   6577   \n",
                            "3136                  9.466661   2967                 12.854729   4093   \n",
                            "3137                 15.788955   2153                 10.606434   2702   \n",
                            "3138                 14.115409    715                  9.067850    903   \n",
                            "3139                 11.181283    681                  9.774652    906   \n",
                            "\n",
                            "      30-39 y/o % of total pop  ...  COPD_number  diabetes_prevalence  \\\n",
                            "0                    12.749771  ...         3644                 12.9   \n",
                            "1                    11.564429  ...        14692                 12.0   \n",
                            "2                    12.865239  ...         2373                 19.7   \n",
                            "3                    13.897321  ...         1789                 14.1   \n",
                            "4                    11.901798  ...         4661                 13.5   \n",
                            "...                        ...  ...          ...                  ...   \n",
                            "3135                 15.277229  ...         2098                  8.9   \n",
                            "3136                 17.733200  ...          928                  7.2   \n",
                            "3137                 13.311001  ...         1163                 10.4   \n",
                            "3138                 11.452124  ...          506                 11.3   \n",
                            "3139                 13.004162  ...          480                 11.7   \n",
                            "\n",
                            "      diabetes_Lower 95% CI  diabetes_Upper 95% CI  diabetes_number  \\\n",
                            "0                      11.9                   13.8             5462   \n",
                            "1                      11.0                   13.1            20520   \n",
                            "2                      18.6                   20.6             3870   \n",
                            "3                      13.2                   14.9             2511   \n",
                            "4                      12.6                   14.5             6017   \n",
                            "...                     ...                    ...              ...   \n",
                            "3135                    8.3                    9.6             2834   \n",
                            "3136                    6.5                    8.0             1360   \n",
                            "3137                    9.5                   11.2             1500   \n",
                            "3138                   10.3                   12.1              686   \n",
                            "3139                   10.7                   12.7              644   \n",
                            "\n",
                            "      CKD_prevalence  CKD_Lower 95% CI  CKD_Upper 95% CI  CKD_number  \\\n",
                            "0                3.1               2.9               3.3        1326   \n",
                            "1                3.2               3.0               3.5        5479   \n",
                            "2                4.5               4.2               4.8         887   \n",
                            "3                3.3               3.1               3.6         595   \n",
                            "4                3.4               3.2               3.7        1507   \n",
                            "...              ...               ...               ...         ...   \n",
                            "3135             2.6               2.4               2.8         821   \n",
                            "3136             2.4               2.2               2.6         447   \n",
                            "3137             3.0               2.8               3.2         430   \n",
                            "3138             3.4               3.2               3.7         207   \n",
                            "3139             3.4               3.1               3.6         185   \n",
                            "\n",
                            "      Urban_rural_code  \n",
                            "0                    3  \n",
                            "1                    4  \n",
                            "2                    6  \n",
                            "3                    2  \n",
                            "4                    2  \n",
                            "...                ...  \n",
                            "3135                 5  \n",
                            "3136                 5  \n",
                            "3137                 5  \n",
                            "3138                 6  \n",
                            "3139                 6  \n",
                            "\n",
                            "[3140 rows x 108 columns]"
                        ]
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "main_df = pd.read_csv(\"https://raw.githubusercontent.com/4GeeksAcademy/regularized-linear-regression-project-tutorial/main/demographic_health_data.csv\")\n",
                "\n",
                "main_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "drop_list = ['COUNTY_NAME', 'STATE_NAME', 'fips', 'Heart disease_prevalence', 'Heart disease_Lower 95% CI', 'Heart disease_Upper 95% CI']\n",
                "\n",
                "main_df = main_df.drop(columns=drop_list)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>TOT_POP</th>\n",
                            "      <th>0-9</th>\n",
                            "      <th>0-9 y/o % of total pop</th>\n",
                            "      <th>19-Oct</th>\n",
                            "      <th>10-19 y/o % of total pop</th>\n",
                            "      <th>20-29</th>\n",
                            "      <th>20-29 y/o % of total pop</th>\n",
                            "      <th>30-39</th>\n",
                            "      <th>30-39 y/o % of total pop</th>\n",
                            "      <th>40-49</th>\n",
                            "      <th>...</th>\n",
                            "      <th>COPD_number</th>\n",
                            "      <th>diabetes_prevalence</th>\n",
                            "      <th>diabetes_Lower 95% CI</th>\n",
                            "      <th>diabetes_Upper 95% CI</th>\n",
                            "      <th>diabetes_number</th>\n",
                            "      <th>CKD_prevalence</th>\n",
                            "      <th>CKD_Lower 95% CI</th>\n",
                            "      <th>CKD_Upper 95% CI</th>\n",
                            "      <th>CKD_number</th>\n",
                            "      <th>Urban_rural_code</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>55601</td>\n",
                            "      <td>6787</td>\n",
                            "      <td>12.206615</td>\n",
                            "      <td>7637</td>\n",
                            "      <td>13.735364</td>\n",
                            "      <td>6878</td>\n",
                            "      <td>12.370281</td>\n",
                            "      <td>7089</td>\n",
                            "      <td>12.749771</td>\n",
                            "      <td>7582</td>\n",
                            "      <td>...</td>\n",
                            "      <td>3644</td>\n",
                            "      <td>12.9</td>\n",
                            "      <td>11.9</td>\n",
                            "      <td>13.8</td>\n",
                            "      <td>5462</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>2.9</td>\n",
                            "      <td>3.3</td>\n",
                            "      <td>1326</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>218022</td>\n",
                            "      <td>24757</td>\n",
                            "      <td>11.355276</td>\n",
                            "      <td>26913</td>\n",
                            "      <td>12.344167</td>\n",
                            "      <td>23579</td>\n",
                            "      <td>10.814964</td>\n",
                            "      <td>25213</td>\n",
                            "      <td>11.564429</td>\n",
                            "      <td>27338</td>\n",
                            "      <td>...</td>\n",
                            "      <td>14692</td>\n",
                            "      <td>12.0</td>\n",
                            "      <td>11.0</td>\n",
                            "      <td>13.1</td>\n",
                            "      <td>20520</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>3.5</td>\n",
                            "      <td>5479</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>24881</td>\n",
                            "      <td>2732</td>\n",
                            "      <td>10.980266</td>\n",
                            "      <td>2960</td>\n",
                            "      <td>11.896628</td>\n",
                            "      <td>3268</td>\n",
                            "      <td>13.134520</td>\n",
                            "      <td>3201</td>\n",
                            "      <td>12.865239</td>\n",
                            "      <td>3074</td>\n",
                            "      <td>...</td>\n",
                            "      <td>2373</td>\n",
                            "      <td>19.7</td>\n",
                            "      <td>18.6</td>\n",
                            "      <td>20.6</td>\n",
                            "      <td>3870</td>\n",
                            "      <td>4.5</td>\n",
                            "      <td>4.2</td>\n",
                            "      <td>4.8</td>\n",
                            "      <td>887</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>22400</td>\n",
                            "      <td>2456</td>\n",
                            "      <td>10.964286</td>\n",
                            "      <td>2596</td>\n",
                            "      <td>11.589286</td>\n",
                            "      <td>3029</td>\n",
                            "      <td>13.522321</td>\n",
                            "      <td>3113</td>\n",
                            "      <td>13.897321</td>\n",
                            "      <td>3038</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1789</td>\n",
                            "      <td>14.1</td>\n",
                            "      <td>13.2</td>\n",
                            "      <td>14.9</td>\n",
                            "      <td>2511</td>\n",
                            "      <td>3.3</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>3.6</td>\n",
                            "      <td>595</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>57840</td>\n",
                            "      <td>7095</td>\n",
                            "      <td>12.266598</td>\n",
                            "      <td>7570</td>\n",
                            "      <td>13.087828</td>\n",
                            "      <td>6742</td>\n",
                            "      <td>11.656293</td>\n",
                            "      <td>6884</td>\n",
                            "      <td>11.901798</td>\n",
                            "      <td>7474</td>\n",
                            "      <td>...</td>\n",
                            "      <td>4661</td>\n",
                            "      <td>13.5</td>\n",
                            "      <td>12.6</td>\n",
                            "      <td>14.5</td>\n",
                            "      <td>6017</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.7</td>\n",
                            "      <td>1507</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3135</th>\n",
                            "      <td>43051</td>\n",
                            "      <td>6104</td>\n",
                            "      <td>14.178532</td>\n",
                            "      <td>6326</td>\n",
                            "      <td>14.694200</td>\n",
                            "      <td>5359</td>\n",
                            "      <td>12.448027</td>\n",
                            "      <td>6577</td>\n",
                            "      <td>15.277229</td>\n",
                            "      <td>5334</td>\n",
                            "      <td>...</td>\n",
                            "      <td>2098</td>\n",
                            "      <td>8.9</td>\n",
                            "      <td>8.3</td>\n",
                            "      <td>9.6</td>\n",
                            "      <td>2834</td>\n",
                            "      <td>2.6</td>\n",
                            "      <td>2.4</td>\n",
                            "      <td>2.8</td>\n",
                            "      <td>821</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3136</th>\n",
                            "      <td>23081</td>\n",
                            "      <td>2384</td>\n",
                            "      <td>10.328842</td>\n",
                            "      <td>2185</td>\n",
                            "      <td>9.466661</td>\n",
                            "      <td>2967</td>\n",
                            "      <td>12.854729</td>\n",
                            "      <td>4093</td>\n",
                            "      <td>17.733200</td>\n",
                            "      <td>3423</td>\n",
                            "      <td>...</td>\n",
                            "      <td>928</td>\n",
                            "      <td>7.2</td>\n",
                            "      <td>6.5</td>\n",
                            "      <td>8.0</td>\n",
                            "      <td>1360</td>\n",
                            "      <td>2.4</td>\n",
                            "      <td>2.2</td>\n",
                            "      <td>2.6</td>\n",
                            "      <td>447</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3137</th>\n",
                            "      <td>20299</td>\n",
                            "      <td>3121</td>\n",
                            "      <td>15.375142</td>\n",
                            "      <td>3205</td>\n",
                            "      <td>15.788955</td>\n",
                            "      <td>2153</td>\n",
                            "      <td>10.606434</td>\n",
                            "      <td>2702</td>\n",
                            "      <td>13.311001</td>\n",
                            "      <td>2390</td>\n",
                            "      <td>...</td>\n",
                            "      <td>1163</td>\n",
                            "      <td>10.4</td>\n",
                            "      <td>9.5</td>\n",
                            "      <td>11.2</td>\n",
                            "      <td>1500</td>\n",
                            "      <td>3.0</td>\n",
                            "      <td>2.8</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>430</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3138</th>\n",
                            "      <td>7885</td>\n",
                            "      <td>858</td>\n",
                            "      <td>10.881420</td>\n",
                            "      <td>1113</td>\n",
                            "      <td>14.115409</td>\n",
                            "      <td>715</td>\n",
                            "      <td>9.067850</td>\n",
                            "      <td>903</td>\n",
                            "      <td>11.452124</td>\n",
                            "      <td>900</td>\n",
                            "      <td>...</td>\n",
                            "      <td>506</td>\n",
                            "      <td>11.3</td>\n",
                            "      <td>10.3</td>\n",
                            "      <td>12.1</td>\n",
                            "      <td>686</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.2</td>\n",
                            "      <td>3.7</td>\n",
                            "      <td>207</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3139</th>\n",
                            "      <td>6967</td>\n",
                            "      <td>780</td>\n",
                            "      <td>11.195637</td>\n",
                            "      <td>779</td>\n",
                            "      <td>11.181283</td>\n",
                            "      <td>681</td>\n",
                            "      <td>9.774652</td>\n",
                            "      <td>906</td>\n",
                            "      <td>13.004162</td>\n",
                            "      <td>734</td>\n",
                            "      <td>...</td>\n",
                            "      <td>480</td>\n",
                            "      <td>11.7</td>\n",
                            "      <td>10.7</td>\n",
                            "      <td>12.7</td>\n",
                            "      <td>644</td>\n",
                            "      <td>3.4</td>\n",
                            "      <td>3.1</td>\n",
                            "      <td>3.6</td>\n",
                            "      <td>185</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>3140 rows × 102 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "      TOT_POP    0-9  0-9 y/o % of total pop  19-Oct  \\\n",
                            "0       55601   6787               12.206615    7637   \n",
                            "1      218022  24757               11.355276   26913   \n",
                            "2       24881   2732               10.980266    2960   \n",
                            "3       22400   2456               10.964286    2596   \n",
                            "4       57840   7095               12.266598    7570   \n",
                            "...       ...    ...                     ...     ...   \n",
                            "3135    43051   6104               14.178532    6326   \n",
                            "3136    23081   2384               10.328842    2185   \n",
                            "3137    20299   3121               15.375142    3205   \n",
                            "3138     7885    858               10.881420    1113   \n",
                            "3139     6967    780               11.195637     779   \n",
                            "\n",
                            "      10-19 y/o % of total pop  20-29  20-29 y/o % of total pop  30-39  \\\n",
                            "0                    13.735364   6878                 12.370281   7089   \n",
                            "1                    12.344167  23579                 10.814964  25213   \n",
                            "2                    11.896628   3268                 13.134520   3201   \n",
                            "3                    11.589286   3029                 13.522321   3113   \n",
                            "4                    13.087828   6742                 11.656293   6884   \n",
                            "...                        ...    ...                       ...    ...   \n",
                            "3135                 14.694200   5359                 12.448027   6577   \n",
                            "3136                  9.466661   2967                 12.854729   4093   \n",
                            "3137                 15.788955   2153                 10.606434   2702   \n",
                            "3138                 14.115409    715                  9.067850    903   \n",
                            "3139                 11.181283    681                  9.774652    906   \n",
                            "\n",
                            "      30-39 y/o % of total pop  40-49  ...  COPD_number  diabetes_prevalence  \\\n",
                            "0                    12.749771   7582  ...         3644                 12.9   \n",
                            "1                    11.564429  27338  ...        14692                 12.0   \n",
                            "2                    12.865239   3074  ...         2373                 19.7   \n",
                            "3                    13.897321   3038  ...         1789                 14.1   \n",
                            "4                    11.901798   7474  ...         4661                 13.5   \n",
                            "...                        ...    ...  ...          ...                  ...   \n",
                            "3135                 15.277229   5334  ...         2098                  8.9   \n",
                            "3136                 17.733200   3423  ...          928                  7.2   \n",
                            "3137                 13.311001   2390  ...         1163                 10.4   \n",
                            "3138                 11.452124    900  ...          506                 11.3   \n",
                            "3139                 13.004162    734  ...          480                 11.7   \n",
                            "\n",
                            "      diabetes_Lower 95% CI  diabetes_Upper 95% CI  diabetes_number  \\\n",
                            "0                      11.9                   13.8             5462   \n",
                            "1                      11.0                   13.1            20520   \n",
                            "2                      18.6                   20.6             3870   \n",
                            "3                      13.2                   14.9             2511   \n",
                            "4                      12.6                   14.5             6017   \n",
                            "...                     ...                    ...              ...   \n",
                            "3135                    8.3                    9.6             2834   \n",
                            "3136                    6.5                    8.0             1360   \n",
                            "3137                    9.5                   11.2             1500   \n",
                            "3138                   10.3                   12.1              686   \n",
                            "3139                   10.7                   12.7              644   \n",
                            "\n",
                            "      CKD_prevalence  CKD_Lower 95% CI  CKD_Upper 95% CI  CKD_number  \\\n",
                            "0                3.1               2.9               3.3        1326   \n",
                            "1                3.2               3.0               3.5        5479   \n",
                            "2                4.5               4.2               4.8         887   \n",
                            "3                3.3               3.1               3.6         595   \n",
                            "4                3.4               3.2               3.7        1507   \n",
                            "...              ...               ...               ...         ...   \n",
                            "3135             2.6               2.4               2.8         821   \n",
                            "3136             2.4               2.2               2.6         447   \n",
                            "3137             3.0               2.8               3.2         430   \n",
                            "3138             3.4               3.2               3.7         207   \n",
                            "3139             3.4               3.1               3.6         185   \n",
                            "\n",
                            "      Urban_rural_code  \n",
                            "0                    3  \n",
                            "1                    4  \n",
                            "2                    6  \n",
                            "3                    2  \n",
                            "4                    2  \n",
                            "...                ...  \n",
                            "3135                 5  \n",
                            "3136                 5  \n",
                            "3137                 5  \n",
                            "3138                 6  \n",
                            "3139                 6  \n",
                            "\n",
                            "[3140 rows x 102 columns]"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "main_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Let's build a model\n",
                "#### Part 1 - Split into X (predictors) and y (target)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": [
                "#### This is my target variable\n",
                "y = main_df['Heart disease_number']\n",
                "\n",
                "#### This is my predictor matrix\n",
                "X = main_df.drop(columns=['Heart disease_number'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(3140, 101)\n",
                        "(3140,)\n"
                    ]
                }
            ],
            "source": [
                "print(X.shape)\n",
                "print(y.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Part 2 - Split into training and testing datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=104)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(2355, 101)\n",
                        "(785, 101)\n",
                        "(2355,)\n",
                        "(785,)\n"
                    ]
                }
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(X_test.shape)\n",
                "print(y_train.shape)\n",
                "print(y_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MODELING\n",
                "\n",
                "#### Let's create three different models:\n",
                "* Linear Regression\n",
                "* LASSO Regression\n",
                "* RIDGE Regression\n",
                "\n",
                "### Part A - standard old Linear Regression "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LinearRegression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [],
            "source": [
                "#instantiate the model\n",
                "linreg_model = LinearRegression()\n",
                "\n",
                "#fit the model with training data\n",
                "linreg_model.fit(X_train, y_train)\n",
                "\n",
                "# make two sets of predictions\n",
                "# training predictions\n",
                "# testing predictions\n",
                "train_preds = linreg_model.predict(X_train)\n",
                "test_preds = linreg_model.predict(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Part A.0 - evaluate the model with appropriate metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training set R2 score: 0.9994537824347787\n",
                        "Mean Absolute Error on training set: 182.64585614052547\n",
                        "Root Mean Squared Error on training set: 383.3451408827789\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Training set R2 score: {r2_score(y_train, train_preds)}\")\n",
                "print(f\"Mean Absolute Error on training set: {mean_absolute_error(y_train, train_preds)}\")\n",
                "print(f\"Root Mean Squared Error on training set: {np.sqrt(mean_squared_error(y_train, train_preds))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing set R2 score: 0.9984551620062206\n",
                        "Mean Absolute Error on testing set: 172.32927479349124\n",
                        "Root Mean Squared Error on testing set: 509.3008871443637\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Testing set R2 score: {r2_score(y_test, test_preds)}\")\n",
                "print(f\"Mean Absolute Error on testing set: {mean_absolute_error(y_test, test_preds)}\")\n",
                "print(f\"Root Mean Squared Error on testing set: {np.sqrt(mean_squared_error(y_test, test_preds))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Axes: xlabel='Heart disease_number'>"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "#visualize how well my predictions line up with the actual values\n",
                "\n",
                "sns.scatterplot(x = y_test, y = test_preds)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PART B - LASSO Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.model_selection import GridSearchCV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.921e+08, tolerance: 6.336e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n"
                    ]
                }
            ],
            "source": [
                "#instantiate the model\n",
                "lasso_model = Lasso(alpha=1, max_iter= 5000)\n",
                "\n",
                "#fit the model\n",
                "lasso_model.fit(X_train, y_train)\n",
                "\n",
                "#make training and testing predictions\n",
                "lasso_train_preds = lasso_model.predict(X_train)\n",
                "lasso_test_preds = lasso_model.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training set R2 score: 0.9994001094042385\n",
                        "Mean Absolute Error on training set: 175.55467520019354\n",
                        "Root Mean Squared Error on training set: 401.73822699221614\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Training set R2 score: {r2_score(y_train, lasso_train_preds)}\")\n",
                "print(f\"Mean Absolute Error on training set: {mean_absolute_error(y_train, lasso_train_preds)}\")\n",
                "print(f\"Root Mean Squared Error on training set: {np.sqrt(mean_squared_error(y_train, lasso_train_preds))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing set R2 score: 0.9966279783744879\n",
                        "Mean Absolute Error on testing set: 177.25366469771714\n",
                        "Root Mean Squared Error on testing set: 752.4508502743332\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Testing set R2 score: {r2_score(y_test, lasso_test_preds)}\")\n",
                "print(f\"Mean Absolute Error on testing set: {mean_absolute_error(y_test, lasso_test_preds)}\")\n",
                "print(f\"Root Mean Squared Error on testing set: {np.sqrt(mean_squared_error(y_test, lasso_test_preds))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Let's try a GridSearch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.479e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.713e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.742e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.656e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.968e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.163e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.727e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.358e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.980e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.717e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.331e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.700e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.788e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.327e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.529e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.854e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.496e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.180e+08, tolerance: 5.370e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.206e+08, tolerance: 3.969e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.320e+08, tolerance: 5.568e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+08, tolerance: 5.424e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/base.py:1474: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
                        "  return fit_method(estimator, *args, **kwargs)\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.269e+08, tolerance: 5.012e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.484e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.719e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.748e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.663e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.973e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.735e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.365e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.499e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.971e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.727e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.044e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.342e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.886e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.710e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.501e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.798e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.067e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.462e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.337e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.538e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.865e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.503e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.184e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.213e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.325e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.608e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.273e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.527e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.765e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.796e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.712e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.019e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.227e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.797e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.425e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.565e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.045e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.778e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.086e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.390e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.726e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.692e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.475e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.343e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.549e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.877e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.513e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.202e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.342e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.288e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.749e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.016e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.027e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.991e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.251e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.328e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.850e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.512e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.668e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.139e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.055e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.961e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.558e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.866e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.155e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.533e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.394e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.948e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.576e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.257e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.275e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.390e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.692e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.346e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.095e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.269e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.364e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.578e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.109e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.833e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.103e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.537e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.210e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.953e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.682e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.172e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.903e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.696e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.986e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.364e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.952e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.607e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.512e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.686e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.098e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.334e+08, tolerance: 5.370e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.404e+08, tolerance: 3.969e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.483e+08, tolerance: 5.568e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.832e+08, tolerance: 5.424e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.504e+08, tolerance: 5.012e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:678: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.921e+08, tolerance: 6.336e+07\n",
                        "  model = cd_fast.enet_coordinate_descent(\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-4 {\n",
                            "  /* Definition of color scheme common for light and dark mode */\n",
                            "  --sklearn-color-text: black;\n",
                            "  --sklearn-color-line: gray;\n",
                            "  /* Definition of color scheme for unfitted estimators */\n",
                            "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
                            "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
                            "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
                            "  --sklearn-color-unfitted-level-3: chocolate;\n",
                            "  /* Definition of color scheme for fitted estimators */\n",
                            "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
                            "  --sklearn-color-fitted-level-1: #d4ebff;\n",
                            "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
                            "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
                            "\n",
                            "  /* Specific color for light theme */\n",
                            "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
                            "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-icon: #696969;\n",
                            "\n",
                            "  @media (prefers-color-scheme: dark) {\n",
                            "    /* Redefinition of color scheme for dark theme */\n",
                            "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
                            "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-icon: #878787;\n",
                            "  }\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 pre {\n",
                            "  padding: 0;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 input.sk-hidden--visually {\n",
                            "  border: 0;\n",
                            "  clip: rect(1px 1px 1px 1px);\n",
                            "  clip: rect(1px, 1px, 1px, 1px);\n",
                            "  height: 1px;\n",
                            "  margin: -1px;\n",
                            "  overflow: hidden;\n",
                            "  padding: 0;\n",
                            "  position: absolute;\n",
                            "  width: 1px;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-dashed-wrapped {\n",
                            "  border: 1px dashed var(--sklearn-color-line);\n",
                            "  margin: 0 0.4em 0.5em 0.4em;\n",
                            "  box-sizing: border-box;\n",
                            "  padding-bottom: 0.4em;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-container {\n",
                            "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
                            "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
                            "     so we also need the `!important` here to be able to override the\n",
                            "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
                            "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
                            "  display: inline-block !important;\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-text-repr-fallback {\n",
                            "  display: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-parallel-item,\n",
                            "div.sk-serial,\n",
                            "div.sk-item {\n",
                            "  /* draw centered vertical line to link estimators */\n",
                            "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
                            "  background-size: 2px 100%;\n",
                            "  background-repeat: no-repeat;\n",
                            "  background-position: center center;\n",
                            "}\n",
                            "\n",
                            "/* Parallel-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel-item::after {\n",
                            "  content: \"\";\n",
                            "  width: 100%;\n",
                            "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
                            "  flex-grow: 1;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel {\n",
                            "  display: flex;\n",
                            "  align-items: stretch;\n",
                            "  justify-content: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel-item {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
                            "  align-self: flex-end;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
                            "  align-self: flex-start;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
                            "  width: 0;\n",
                            "}\n",
                            "\n",
                            "/* Serial-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-serial {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "  align-items: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  padding-right: 1em;\n",
                            "  padding-left: 1em;\n",
                            "}\n",
                            "\n",
                            "\n",
                            "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
                            "clickable and can be expanded/collapsed.\n",
                            "- Pipeline and ColumnTransformer use this feature and define the default style\n",
                            "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
                            "*/\n",
                            "\n",
                            "/* Pipeline and ColumnTransformer style (default) */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-toggleable {\n",
                            "  /* Default theme specific background. It is overwritten whether we have a\n",
                            "  specific estimator or a Pipeline/ColumnTransformer */\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable label */\n",
                            "#sk-container-id-4 label.sk-toggleable__label {\n",
                            "  cursor: pointer;\n",
                            "  display: block;\n",
                            "  width: 100%;\n",
                            "  margin-bottom: 0;\n",
                            "  padding: 0.5em;\n",
                            "  box-sizing: border-box;\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
                            "  /* Arrow on the left of the label */\n",
                            "  content: \"▸\";\n",
                            "  float: left;\n",
                            "  margin-right: 0.25em;\n",
                            "  color: var(--sklearn-color-icon);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable content - dropdown */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-toggleable__content {\n",
                            "  max-height: 0;\n",
                            "  max-width: 0;\n",
                            "  overflow: hidden;\n",
                            "  text-align: left;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-toggleable__content pre {\n",
                            "  margin: 0.2em;\n",
                            "  border-radius: 0.25em;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
                            "  /* Expand drop-down */\n",
                            "  max-height: 200px;\n",
                            "  max-width: 100%;\n",
                            "  overflow: auto;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
                            "  content: \"▾\";\n",
                            "}\n",
                            "\n",
                            "/* Pipeline/ColumnTransformer-specific style */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific style */\n",
                            "\n",
                            "/* Colorize estimator box */\n",
                            "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
                            "#sk-container-id-4 div.sk-label label {\n",
                            "  /* The background is the default theme color */\n",
                            "  color: var(--sklearn-color-text-on-default-background);\n",
                            "}\n",
                            "\n",
                            "/* On hover, darken the color of the background */\n",
                            "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Label box, darken color on hover, fitted */\n",
                            "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator label */\n",
                            "\n",
                            "#sk-container-id-4 div.sk-label label {\n",
                            "  font-family: monospace;\n",
                            "  font-weight: bold;\n",
                            "  display: inline-block;\n",
                            "  line-height: 1.2em;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-label-container {\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific */\n",
                            "#sk-container-id-4 div.sk-estimator {\n",
                            "  font-family: monospace;\n",
                            "  border: 1px dotted var(--sklearn-color-border-box);\n",
                            "  border-radius: 0.25em;\n",
                            "  box-sizing: border-box;\n",
                            "  margin-bottom: 0.5em;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-estimator.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "/* on hover */\n",
                            "#sk-container-id-4 div.sk-estimator:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
                            "\n",
                            "/* Common style for \"i\" and \"?\" */\n",
                            "\n",
                            ".sk-estimator-doc-link,\n",
                            "a:link.sk-estimator-doc-link,\n",
                            "a:visited.sk-estimator-doc-link {\n",
                            "  float: right;\n",
                            "  font-size: smaller;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1em;\n",
                            "  height: 1em;\n",
                            "  width: 1em;\n",
                            "  text-decoration: none !important;\n",
                            "  margin-left: 1ex;\n",
                            "  /* unfitted */\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted,\n",
                            "a:link.sk-estimator-doc-link.fitted,\n",
                            "a:visited.sk-estimator-doc-link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "/* Span, style for the box shown on hovering the info icon */\n",
                            ".sk-estimator-doc-link span {\n",
                            "  display: none;\n",
                            "  z-index: 9999;\n",
                            "  position: relative;\n",
                            "  font-weight: normal;\n",
                            "  right: .2ex;\n",
                            "  padding: .5ex;\n",
                            "  margin: .5ex;\n",
                            "  width: min-content;\n",
                            "  min-width: 20ex;\n",
                            "  max-width: 50ex;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  box-shadow: 2pt 2pt 4pt #999;\n",
                            "  /* unfitted */\n",
                            "  background: var(--sklearn-color-unfitted-level-0);\n",
                            "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted span {\n",
                            "  /* fitted */\n",
                            "  background: var(--sklearn-color-fitted-level-0);\n",
                            "  border: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link:hover span {\n",
                            "  display: block;\n",
                            "}\n",
                            "\n",
                            "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
                            "\n",
                            "#sk-container-id-4 a.estimator_doc_link {\n",
                            "  float: right;\n",
                            "  font-size: 1rem;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1rem;\n",
                            "  height: 1rem;\n",
                            "  width: 1rem;\n",
                            "  text-decoration: none;\n",
                            "  /* unfitted */\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "#sk-container-id-4 a.estimator_doc_link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Lasso(),\n",
                            "             param_grid={&#x27;alpha&#x27;: [0, 0.1, 1, 10, 100],\n",
                            "                         &#x27;max_iter&#x27;: [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=Lasso(),\n",
                            "             param_grid={&#x27;alpha&#x27;: [0, 0.1, 1, 10, 100],\n",
                            "                         &#x27;max_iter&#x27;: [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring=&#x27;r2&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Lasso</label><div class=\"sk-toggleable__content fitted\"><pre>Lasso()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Lasso<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.Lasso.html\">?<span>Documentation for Lasso</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>Lasso()</pre></div> </div></div></div></div></div></div></div></div></div>"
                        ],
                        "text/plain": [
                            "GridSearchCV(estimator=Lasso(),\n",
                            "             param_grid={'alpha': [0, 0.1, 1, 10, 100],\n",
                            "                         'max_iter': [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring='r2')"
                        ]
                    },
                    "execution_count": 83,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "hyperparameters = {'alpha' : [0, 0.1, 1, 10, 100],\n",
                "                   'max_iter' : [50, 100, 500, 1000, 2000, 5000],\n",
                "                   }\n",
                "\n",
                "grid_model = GridSearchCV(Lasso(), param_grid=hyperparameters, scoring='r2')\n",
                "                          \n",
                "grid_model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 84,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'alpha': 1, 'max_iter': 5000}"
                        ]
                    },
                    "execution_count": 84,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid_model.best_params_"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>mean_fit_time</th>\n",
                            "      <th>std_fit_time</th>\n",
                            "      <th>mean_score_time</th>\n",
                            "      <th>std_score_time</th>\n",
                            "      <th>param_alpha</th>\n",
                            "      <th>param_max_iter</th>\n",
                            "      <th>params</th>\n",
                            "      <th>split0_test_score</th>\n",
                            "      <th>split1_test_score</th>\n",
                            "      <th>split2_test_score</th>\n",
                            "      <th>split3_test_score</th>\n",
                            "      <th>split4_test_score</th>\n",
                            "      <th>mean_test_score</th>\n",
                            "      <th>std_test_score</th>\n",
                            "      <th>rank_test_score</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0.034199</td>\n",
                            "      <td>0.015238</td>\n",
                            "      <td>0.010095</td>\n",
                            "      <td>0.002235</td>\n",
                            "      <td>0</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 50}</td>\n",
                            "      <td>0.995656</td>\n",
                            "      <td>0.997177</td>\n",
                            "      <td>0.995446</td>\n",
                            "      <td>0.997306</td>\n",
                            "      <td>0.995783</td>\n",
                            "      <td>0.996273</td>\n",
                            "      <td>0.000799</td>\n",
                            "      <td>29</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0.033844</td>\n",
                            "      <td>0.008627</td>\n",
                            "      <td>0.005694</td>\n",
                            "      <td>0.003759</td>\n",
                            "      <td>0</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 100}</td>\n",
                            "      <td>0.996185</td>\n",
                            "      <td>0.997467</td>\n",
                            "      <td>0.996824</td>\n",
                            "      <td>0.997279</td>\n",
                            "      <td>0.996409</td>\n",
                            "      <td>0.996833</td>\n",
                            "      <td>0.000490</td>\n",
                            "      <td>9</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0.109072</td>\n",
                            "      <td>0.021035</td>\n",
                            "      <td>0.004203</td>\n",
                            "      <td>0.002718</td>\n",
                            "      <td>0</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 500}</td>\n",
                            "      <td>0.997644</td>\n",
                            "      <td>0.995357</td>\n",
                            "      <td>0.996842</td>\n",
                            "      <td>0.997156</td>\n",
                            "      <td>0.996861</td>\n",
                            "      <td>0.996772</td>\n",
                            "      <td>0.000765</td>\n",
                            "      <td>12</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0.200582</td>\n",
                            "      <td>0.022088</td>\n",
                            "      <td>0.006358</td>\n",
                            "      <td>0.002273</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 1000}</td>\n",
                            "      <td>0.998074</td>\n",
                            "      <td>0.993982</td>\n",
                            "      <td>0.996630</td>\n",
                            "      <td>0.997457</td>\n",
                            "      <td>0.997139</td>\n",
                            "      <td>0.996656</td>\n",
                            "      <td>0.001417</td>\n",
                            "      <td>20</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0.353155</td>\n",
                            "      <td>0.028411</td>\n",
                            "      <td>0.005054</td>\n",
                            "      <td>0.001804</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 2000}</td>\n",
                            "      <td>0.998085</td>\n",
                            "      <td>0.993167</td>\n",
                            "      <td>0.996898</td>\n",
                            "      <td>0.997809</td>\n",
                            "      <td>0.997714</td>\n",
                            "      <td>0.996735</td>\n",
                            "      <td>0.001827</td>\n",
                            "      <td>15</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>0.904181</td>\n",
                            "      <td>0.064036</td>\n",
                            "      <td>0.004611</td>\n",
                            "      <td>0.002000</td>\n",
                            "      <td>0</td>\n",
                            "      <td>5000</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 5000}</td>\n",
                            "      <td>0.997774</td>\n",
                            "      <td>0.993721</td>\n",
                            "      <td>0.997174</td>\n",
                            "      <td>0.998548</td>\n",
                            "      <td>0.998093</td>\n",
                            "      <td>0.997062</td>\n",
                            "      <td>0.001729</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>0.014035</td>\n",
                            "      <td>0.002373</td>\n",
                            "      <td>0.004327</td>\n",
                            "      <td>0.003018</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 50}</td>\n",
                            "      <td>0.995663</td>\n",
                            "      <td>0.997176</td>\n",
                            "      <td>0.995448</td>\n",
                            "      <td>0.997307</td>\n",
                            "      <td>0.995783</td>\n",
                            "      <td>0.996275</td>\n",
                            "      <td>0.000797</td>\n",
                            "      <td>28</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>7</th>\n",
                            "      <td>0.039244</td>\n",
                            "      <td>0.005650</td>\n",
                            "      <td>0.006586</td>\n",
                            "      <td>0.002198</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 100}</td>\n",
                            "      <td>0.996193</td>\n",
                            "      <td>0.997467</td>\n",
                            "      <td>0.996826</td>\n",
                            "      <td>0.997281</td>\n",
                            "      <td>0.996410</td>\n",
                            "      <td>0.996835</td>\n",
                            "      <td>0.000488</td>\n",
                            "      <td>8</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>0.110565</td>\n",
                            "      <td>0.017593</td>\n",
                            "      <td>0.003604</td>\n",
                            "      <td>0.001596</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 500}</td>\n",
                            "      <td>0.997643</td>\n",
                            "      <td>0.995354</td>\n",
                            "      <td>0.996843</td>\n",
                            "      <td>0.997163</td>\n",
                            "      <td>0.996859</td>\n",
                            "      <td>0.996772</td>\n",
                            "      <td>0.000766</td>\n",
                            "      <td>11</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>0.204935</td>\n",
                            "      <td>0.043594</td>\n",
                            "      <td>0.006688</td>\n",
                            "      <td>0.002255</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 1000}</td>\n",
                            "      <td>0.998075</td>\n",
                            "      <td>0.993984</td>\n",
                            "      <td>0.996627</td>\n",
                            "      <td>0.997465</td>\n",
                            "      <td>0.997136</td>\n",
                            "      <td>0.996657</td>\n",
                            "      <td>0.001417</td>\n",
                            "      <td>19</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>10</th>\n",
                            "      <td>0.343116</td>\n",
                            "      <td>0.016801</td>\n",
                            "      <td>0.005651</td>\n",
                            "      <td>0.001037</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 2000}</td>\n",
                            "      <td>0.998087</td>\n",
                            "      <td>0.993174</td>\n",
                            "      <td>0.996898</td>\n",
                            "      <td>0.997819</td>\n",
                            "      <td>0.997715</td>\n",
                            "      <td>0.996739</td>\n",
                            "      <td>0.001826</td>\n",
                            "      <td>14</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>11</th>\n",
                            "      <td>0.850948</td>\n",
                            "      <td>0.044001</td>\n",
                            "      <td>0.006806</td>\n",
                            "      <td>0.002734</td>\n",
                            "      <td>0.1</td>\n",
                            "      <td>5000</td>\n",
                            "      <td>{'alpha': 0.1, 'max_iter': 5000}</td>\n",
                            "      <td>0.997777</td>\n",
                            "      <td>0.993724</td>\n",
                            "      <td>0.997180</td>\n",
                            "      <td>0.998558</td>\n",
                            "      <td>0.998092</td>\n",
                            "      <td>0.997066</td>\n",
                            "      <td>0.001730</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>12</th>\n",
                            "      <td>0.021078</td>\n",
                            "      <td>0.005716</td>\n",
                            "      <td>0.007034</td>\n",
                            "      <td>0.003768</td>\n",
                            "      <td>1</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 50}</td>\n",
                            "      <td>0.995723</td>\n",
                            "      <td>0.997173</td>\n",
                            "      <td>0.995464</td>\n",
                            "      <td>0.997317</td>\n",
                            "      <td>0.995786</td>\n",
                            "      <td>0.996292</td>\n",
                            "      <td>0.000786</td>\n",
                            "      <td>27</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>0.024670</td>\n",
                            "      <td>0.005964</td>\n",
                            "      <td>0.011387</td>\n",
                            "      <td>0.005555</td>\n",
                            "      <td>1</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 100}</td>\n",
                            "      <td>0.996255</td>\n",
                            "      <td>0.997468</td>\n",
                            "      <td>0.996841</td>\n",
                            "      <td>0.997295</td>\n",
                            "      <td>0.996412</td>\n",
                            "      <td>0.996854</td>\n",
                            "      <td>0.000474</td>\n",
                            "      <td>6</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>14</th>\n",
                            "      <td>0.099387</td>\n",
                            "      <td>0.018468</td>\n",
                            "      <td>0.003993</td>\n",
                            "      <td>0.001816</td>\n",
                            "      <td>1</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 500}</td>\n",
                            "      <td>0.997654</td>\n",
                            "      <td>0.995339</td>\n",
                            "      <td>0.996840</td>\n",
                            "      <td>0.997221</td>\n",
                            "      <td>0.996861</td>\n",
                            "      <td>0.996783</td>\n",
                            "      <td>0.000780</td>\n",
                            "      <td>10</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>15</th>\n",
                            "      <td>0.174294</td>\n",
                            "      <td>0.017369</td>\n",
                            "      <td>0.004921</td>\n",
                            "      <td>0.001835</td>\n",
                            "      <td>1</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 1000}</td>\n",
                            "      <td>0.998087</td>\n",
                            "      <td>0.993931</td>\n",
                            "      <td>0.996619</td>\n",
                            "      <td>0.997517</td>\n",
                            "      <td>0.997148</td>\n",
                            "      <td>0.996660</td>\n",
                            "      <td>0.001446</td>\n",
                            "      <td>18</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>16</th>\n",
                            "      <td>0.329729</td>\n",
                            "      <td>0.038319</td>\n",
                            "      <td>0.002743</td>\n",
                            "      <td>0.000182</td>\n",
                            "      <td>1</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 2000}</td>\n",
                            "      <td>0.998109</td>\n",
                            "      <td>0.993160</td>\n",
                            "      <td>0.996896</td>\n",
                            "      <td>0.997853</td>\n",
                            "      <td>0.997712</td>\n",
                            "      <td>0.996746</td>\n",
                            "      <td>0.001838</td>\n",
                            "      <td>13</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>17</th>\n",
                            "      <td>0.877133</td>\n",
                            "      <td>0.034872</td>\n",
                            "      <td>0.005105</td>\n",
                            "      <td>0.001174</td>\n",
                            "      <td>1</td>\n",
                            "      <td>5000</td>\n",
                            "      <td>{'alpha': 1, 'max_iter': 5000}</td>\n",
                            "      <td>0.997832</td>\n",
                            "      <td>0.993726</td>\n",
                            "      <td>0.997199</td>\n",
                            "      <td>0.998538</td>\n",
                            "      <td>0.998080</td>\n",
                            "      <td>0.997075</td>\n",
                            "      <td>0.001729</td>\n",
                            "      <td>1</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>18</th>\n",
                            "      <td>0.012536</td>\n",
                            "      <td>0.000500</td>\n",
                            "      <td>0.003633</td>\n",
                            "      <td>0.001215</td>\n",
                            "      <td>10</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 50}</td>\n",
                            "      <td>0.995836</td>\n",
                            "      <td>0.997151</td>\n",
                            "      <td>0.995474</td>\n",
                            "      <td>0.997370</td>\n",
                            "      <td>0.995732</td>\n",
                            "      <td>0.996312</td>\n",
                            "      <td>0.000786</td>\n",
                            "      <td>26</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19</th>\n",
                            "      <td>0.039494</td>\n",
                            "      <td>0.009166</td>\n",
                            "      <td>0.009650</td>\n",
                            "      <td>0.001812</td>\n",
                            "      <td>10</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 100}</td>\n",
                            "      <td>0.996343</td>\n",
                            "      <td>0.997470</td>\n",
                            "      <td>0.996808</td>\n",
                            "      <td>0.997347</td>\n",
                            "      <td>0.996248</td>\n",
                            "      <td>0.996843</td>\n",
                            "      <td>0.000500</td>\n",
                            "      <td>7</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>20</th>\n",
                            "      <td>0.094453</td>\n",
                            "      <td>0.013804</td>\n",
                            "      <td>0.004154</td>\n",
                            "      <td>0.001803</td>\n",
                            "      <td>10</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 500}</td>\n",
                            "      <td>0.997620</td>\n",
                            "      <td>0.995147</td>\n",
                            "      <td>0.996732</td>\n",
                            "      <td>0.997258</td>\n",
                            "      <td>0.996811</td>\n",
                            "      <td>0.996714</td>\n",
                            "      <td>0.000847</td>\n",
                            "      <td>17</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>21</th>\n",
                            "      <td>0.171343</td>\n",
                            "      <td>0.023257</td>\n",
                            "      <td>0.006533</td>\n",
                            "      <td>0.002104</td>\n",
                            "      <td>10</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 1000}</td>\n",
                            "      <td>0.998062</td>\n",
                            "      <td>0.993821</td>\n",
                            "      <td>0.996522</td>\n",
                            "      <td>0.997520</td>\n",
                            "      <td>0.997129</td>\n",
                            "      <td>0.996611</td>\n",
                            "      <td>0.001483</td>\n",
                            "      <td>22</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>22</th>\n",
                            "      <td>0.275790</td>\n",
                            "      <td>0.017237</td>\n",
                            "      <td>0.005001</td>\n",
                            "      <td>0.002391</td>\n",
                            "      <td>10</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 2000}</td>\n",
                            "      <td>0.998071</td>\n",
                            "      <td>0.993134</td>\n",
                            "      <td>0.996829</td>\n",
                            "      <td>0.997845</td>\n",
                            "      <td>0.997731</td>\n",
                            "      <td>0.996722</td>\n",
                            "      <td>0.001843</td>\n",
                            "      <td>16</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>23</th>\n",
                            "      <td>0.731214</td>\n",
                            "      <td>0.064503</td>\n",
                            "      <td>0.007597</td>\n",
                            "      <td>0.007614</td>\n",
                            "      <td>10</td>\n",
                            "      <td>5000</td>\n",
                            "      <td>{'alpha': 10, 'max_iter': 5000}</td>\n",
                            "      <td>0.997852</td>\n",
                            "      <td>0.993662</td>\n",
                            "      <td>0.997150</td>\n",
                            "      <td>0.998531</td>\n",
                            "      <td>0.998122</td>\n",
                            "      <td>0.997063</td>\n",
                            "      <td>0.001759</td>\n",
                            "      <td>3</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>24</th>\n",
                            "      <td>0.011655</td>\n",
                            "      <td>0.001033</td>\n",
                            "      <td>0.002774</td>\n",
                            "      <td>0.000412</td>\n",
                            "      <td>100</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 50}</td>\n",
                            "      <td>0.995300</td>\n",
                            "      <td>0.997022</td>\n",
                            "      <td>0.995355</td>\n",
                            "      <td>0.997376</td>\n",
                            "      <td>0.995001</td>\n",
                            "      <td>0.996011</td>\n",
                            "      <td>0.000984</td>\n",
                            "      <td>30</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>25</th>\n",
                            "      <td>0.018363</td>\n",
                            "      <td>0.003514</td>\n",
                            "      <td>0.004942</td>\n",
                            "      <td>0.001702</td>\n",
                            "      <td>100</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 100}</td>\n",
                            "      <td>0.995545</td>\n",
                            "      <td>0.997083</td>\n",
                            "      <td>0.996504</td>\n",
                            "      <td>0.997202</td>\n",
                            "      <td>0.995863</td>\n",
                            "      <td>0.996439</td>\n",
                            "      <td>0.000653</td>\n",
                            "      <td>25</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>26</th>\n",
                            "      <td>0.092211</td>\n",
                            "      <td>0.018449</td>\n",
                            "      <td>0.007021</td>\n",
                            "      <td>0.003654</td>\n",
                            "      <td>100</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 500}</td>\n",
                            "      <td>0.997458</td>\n",
                            "      <td>0.994859</td>\n",
                            "      <td>0.996347</td>\n",
                            "      <td>0.997055</td>\n",
                            "      <td>0.996778</td>\n",
                            "      <td>0.996499</td>\n",
                            "      <td>0.000897</td>\n",
                            "      <td>23</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>27</th>\n",
                            "      <td>0.140926</td>\n",
                            "      <td>0.021814</td>\n",
                            "      <td>0.004497</td>\n",
                            "      <td>0.002025</td>\n",
                            "      <td>100</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 1000}</td>\n",
                            "      <td>0.997950</td>\n",
                            "      <td>0.993620</td>\n",
                            "      <td>0.996283</td>\n",
                            "      <td>0.997451</td>\n",
                            "      <td>0.997150</td>\n",
                            "      <td>0.996491</td>\n",
                            "      <td>0.001534</td>\n",
                            "      <td>24</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>28</th>\n",
                            "      <td>0.303074</td>\n",
                            "      <td>0.082991</td>\n",
                            "      <td>0.004602</td>\n",
                            "      <td>0.002034</td>\n",
                            "      <td>100</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 2000}</td>\n",
                            "      <td>0.997911</td>\n",
                            "      <td>0.992940</td>\n",
                            "      <td>0.996686</td>\n",
                            "      <td>0.997821</td>\n",
                            "      <td>0.997845</td>\n",
                            "      <td>0.996640</td>\n",
                            "      <td>0.001906</td>\n",
                            "      <td>21</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>29</th>\n",
                            "      <td>0.618733</td>\n",
                            "      <td>0.102090</td>\n",
                            "      <td>0.002845</td>\n",
                            "      <td>0.000487</td>\n",
                            "      <td>100</td>\n",
                            "      <td>5000</td>\n",
                            "      <td>{'alpha': 100, 'max_iter': 5000}</td>\n",
                            "      <td>0.997680</td>\n",
                            "      <td>0.993533</td>\n",
                            "      <td>0.996978</td>\n",
                            "      <td>0.998662</td>\n",
                            "      <td>0.998215</td>\n",
                            "      <td>0.997014</td>\n",
                            "      <td>0.001829</td>\n",
                            "      <td>5</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
                            "0        0.034199      0.015238         0.010095        0.002235           0   \n",
                            "1        0.033844      0.008627         0.005694        0.003759           0   \n",
                            "2        0.109072      0.021035         0.004203        0.002718           0   \n",
                            "3        0.200582      0.022088         0.006358        0.002273           0   \n",
                            "4        0.353155      0.028411         0.005054        0.001804           0   \n",
                            "5        0.904181      0.064036         0.004611        0.002000           0   \n",
                            "6        0.014035      0.002373         0.004327        0.003018         0.1   \n",
                            "7        0.039244      0.005650         0.006586        0.002198         0.1   \n",
                            "8        0.110565      0.017593         0.003604        0.001596         0.1   \n",
                            "9        0.204935      0.043594         0.006688        0.002255         0.1   \n",
                            "10       0.343116      0.016801         0.005651        0.001037         0.1   \n",
                            "11       0.850948      0.044001         0.006806        0.002734         0.1   \n",
                            "12       0.021078      0.005716         0.007034        0.003768           1   \n",
                            "13       0.024670      0.005964         0.011387        0.005555           1   \n",
                            "14       0.099387      0.018468         0.003993        0.001816           1   \n",
                            "15       0.174294      0.017369         0.004921        0.001835           1   \n",
                            "16       0.329729      0.038319         0.002743        0.000182           1   \n",
                            "17       0.877133      0.034872         0.005105        0.001174           1   \n",
                            "18       0.012536      0.000500         0.003633        0.001215          10   \n",
                            "19       0.039494      0.009166         0.009650        0.001812          10   \n",
                            "20       0.094453      0.013804         0.004154        0.001803          10   \n",
                            "21       0.171343      0.023257         0.006533        0.002104          10   \n",
                            "22       0.275790      0.017237         0.005001        0.002391          10   \n",
                            "23       0.731214      0.064503         0.007597        0.007614          10   \n",
                            "24       0.011655      0.001033         0.002774        0.000412         100   \n",
                            "25       0.018363      0.003514         0.004942        0.001702         100   \n",
                            "26       0.092211      0.018449         0.007021        0.003654         100   \n",
                            "27       0.140926      0.021814         0.004497        0.002025         100   \n",
                            "28       0.303074      0.082991         0.004602        0.002034         100   \n",
                            "29       0.618733      0.102090         0.002845        0.000487         100   \n",
                            "\n",
                            "   param_max_iter                            params  split0_test_score  \\\n",
                            "0              50      {'alpha': 0, 'max_iter': 50}           0.995656   \n",
                            "1             100     {'alpha': 0, 'max_iter': 100}           0.996185   \n",
                            "2             500     {'alpha': 0, 'max_iter': 500}           0.997644   \n",
                            "3            1000    {'alpha': 0, 'max_iter': 1000}           0.998074   \n",
                            "4            2000    {'alpha': 0, 'max_iter': 2000}           0.998085   \n",
                            "5            5000    {'alpha': 0, 'max_iter': 5000}           0.997774   \n",
                            "6              50    {'alpha': 0.1, 'max_iter': 50}           0.995663   \n",
                            "7             100   {'alpha': 0.1, 'max_iter': 100}           0.996193   \n",
                            "8             500   {'alpha': 0.1, 'max_iter': 500}           0.997643   \n",
                            "9            1000  {'alpha': 0.1, 'max_iter': 1000}           0.998075   \n",
                            "10           2000  {'alpha': 0.1, 'max_iter': 2000}           0.998087   \n",
                            "11           5000  {'alpha': 0.1, 'max_iter': 5000}           0.997777   \n",
                            "12             50      {'alpha': 1, 'max_iter': 50}           0.995723   \n",
                            "13            100     {'alpha': 1, 'max_iter': 100}           0.996255   \n",
                            "14            500     {'alpha': 1, 'max_iter': 500}           0.997654   \n",
                            "15           1000    {'alpha': 1, 'max_iter': 1000}           0.998087   \n",
                            "16           2000    {'alpha': 1, 'max_iter': 2000}           0.998109   \n",
                            "17           5000    {'alpha': 1, 'max_iter': 5000}           0.997832   \n",
                            "18             50     {'alpha': 10, 'max_iter': 50}           0.995836   \n",
                            "19            100    {'alpha': 10, 'max_iter': 100}           0.996343   \n",
                            "20            500    {'alpha': 10, 'max_iter': 500}           0.997620   \n",
                            "21           1000   {'alpha': 10, 'max_iter': 1000}           0.998062   \n",
                            "22           2000   {'alpha': 10, 'max_iter': 2000}           0.998071   \n",
                            "23           5000   {'alpha': 10, 'max_iter': 5000}           0.997852   \n",
                            "24             50    {'alpha': 100, 'max_iter': 50}           0.995300   \n",
                            "25            100   {'alpha': 100, 'max_iter': 100}           0.995545   \n",
                            "26            500   {'alpha': 100, 'max_iter': 500}           0.997458   \n",
                            "27           1000  {'alpha': 100, 'max_iter': 1000}           0.997950   \n",
                            "28           2000  {'alpha': 100, 'max_iter': 2000}           0.997911   \n",
                            "29           5000  {'alpha': 100, 'max_iter': 5000}           0.997680   \n",
                            "\n",
                            "    split1_test_score  split2_test_score  split3_test_score  \\\n",
                            "0            0.997177           0.995446           0.997306   \n",
                            "1            0.997467           0.996824           0.997279   \n",
                            "2            0.995357           0.996842           0.997156   \n",
                            "3            0.993982           0.996630           0.997457   \n",
                            "4            0.993167           0.996898           0.997809   \n",
                            "5            0.993721           0.997174           0.998548   \n",
                            "6            0.997176           0.995448           0.997307   \n",
                            "7            0.997467           0.996826           0.997281   \n",
                            "8            0.995354           0.996843           0.997163   \n",
                            "9            0.993984           0.996627           0.997465   \n",
                            "10           0.993174           0.996898           0.997819   \n",
                            "11           0.993724           0.997180           0.998558   \n",
                            "12           0.997173           0.995464           0.997317   \n",
                            "13           0.997468           0.996841           0.997295   \n",
                            "14           0.995339           0.996840           0.997221   \n",
                            "15           0.993931           0.996619           0.997517   \n",
                            "16           0.993160           0.996896           0.997853   \n",
                            "17           0.993726           0.997199           0.998538   \n",
                            "18           0.997151           0.995474           0.997370   \n",
                            "19           0.997470           0.996808           0.997347   \n",
                            "20           0.995147           0.996732           0.997258   \n",
                            "21           0.993821           0.996522           0.997520   \n",
                            "22           0.993134           0.996829           0.997845   \n",
                            "23           0.993662           0.997150           0.998531   \n",
                            "24           0.997022           0.995355           0.997376   \n",
                            "25           0.997083           0.996504           0.997202   \n",
                            "26           0.994859           0.996347           0.997055   \n",
                            "27           0.993620           0.996283           0.997451   \n",
                            "28           0.992940           0.996686           0.997821   \n",
                            "29           0.993533           0.996978           0.998662   \n",
                            "\n",
                            "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
                            "0            0.995783         0.996273        0.000799               29  \n",
                            "1            0.996409         0.996833        0.000490                9  \n",
                            "2            0.996861         0.996772        0.000765               12  \n",
                            "3            0.997139         0.996656        0.001417               20  \n",
                            "4            0.997714         0.996735        0.001827               15  \n",
                            "5            0.998093         0.997062        0.001729                4  \n",
                            "6            0.995783         0.996275        0.000797               28  \n",
                            "7            0.996410         0.996835        0.000488                8  \n",
                            "8            0.996859         0.996772        0.000766               11  \n",
                            "9            0.997136         0.996657        0.001417               19  \n",
                            "10           0.997715         0.996739        0.001826               14  \n",
                            "11           0.998092         0.997066        0.001730                2  \n",
                            "12           0.995786         0.996292        0.000786               27  \n",
                            "13           0.996412         0.996854        0.000474                6  \n",
                            "14           0.996861         0.996783        0.000780               10  \n",
                            "15           0.997148         0.996660        0.001446               18  \n",
                            "16           0.997712         0.996746        0.001838               13  \n",
                            "17           0.998080         0.997075        0.001729                1  \n",
                            "18           0.995732         0.996312        0.000786               26  \n",
                            "19           0.996248         0.996843        0.000500                7  \n",
                            "20           0.996811         0.996714        0.000847               17  \n",
                            "21           0.997129         0.996611        0.001483               22  \n",
                            "22           0.997731         0.996722        0.001843               16  \n",
                            "23           0.998122         0.997063        0.001759                3  \n",
                            "24           0.995001         0.996011        0.000984               30  \n",
                            "25           0.995863         0.996439        0.000653               25  \n",
                            "26           0.996778         0.996499        0.000897               23  \n",
                            "27           0.997150         0.996491        0.001534               24  \n",
                            "28           0.997845         0.996640        0.001906               21  \n",
                            "29           0.998215         0.997014        0.001829                5  "
                        ]
                    },
                    "execution_count": 85,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pd.DataFrame(grid_model.cv_results_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>0</th>\n",
                            "      <th>1</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>TOT_POP</td>\n",
                            "      <td>0.022602</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0-9</td>\n",
                            "      <td>0.086924</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0-9 y/o % of total pop</td>\n",
                            "      <td>-38.619933</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>19-Oct</td>\n",
                            "      <td>-0.104912</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>10-19 y/o % of total pop</td>\n",
                            "      <td>18.689033</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>96</th>\n",
                            "      <td>CKD_prevalence</td>\n",
                            "      <td>230.932713</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>97</th>\n",
                            "      <td>CKD_Lower 95% CI</td>\n",
                            "      <td>133.424567</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>98</th>\n",
                            "      <td>CKD_Upper 95% CI</td>\n",
                            "      <td>0.0</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>99</th>\n",
                            "      <td>CKD_number</td>\n",
                            "      <td>-0.393764</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>100</th>\n",
                            "      <td>Urban_rural_code</td>\n",
                            "      <td>-10.973017</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>101 rows × 2 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                            0           1\n",
                            "0                     TOT_POP    0.022602\n",
                            "1                         0-9    0.086924\n",
                            "2      0-9 y/o % of total pop  -38.619933\n",
                            "3                      19-Oct   -0.104912\n",
                            "4    10-19 y/o % of total pop   18.689033\n",
                            "..                        ...         ...\n",
                            "96             CKD_prevalence  230.932713\n",
                            "97           CKD_Lower 95% CI  133.424567\n",
                            "98           CKD_Upper 95% CI         0.0\n",
                            "99                 CKD_number   -0.393764\n",
                            "100          Urban_rural_code  -10.973017\n",
                            "\n",
                            "[101 rows x 2 columns]"
                        ]
                    },
                    "execution_count": 86,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pd.DataFrame([lasso_model.feature_names_in_, lasso_model.coef_]).T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PART C - RIDGE Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import Ridge"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge_model = Ridge(alpha=100, max_iter=50)\n",
                "\n",
                "ridge_model.fit(X_train, y_train)\n",
                "\n",
                "ridge_train_preds = ridge_model.predict(X_train)\n",
                "ridge_test_preds = ridge_model.predict(X_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training set R2 score: 0.99946414463355\n",
                        "Mean Absolute Error on training set: 167.89355775378402\n",
                        "Root Mean Squared Error on training set: 379.6915428513819\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Training set R2 score: {r2_score(y_train, ridge_train_preds)}\")\n",
                "print(f\"Mean Absolute Error on training set: {mean_absolute_error(y_train, ridge_train_preds)}\")\n",
                "print(f\"Root Mean Squared Error on training set: {np.sqrt(mean_squared_error(y_train, ridge_train_preds))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Testing set R2 score: 0.9984744345234775\n",
                        "Mean Absolute Error on testing set: 161.97462009800662\n",
                        "Root Mean Squared Error on testing set: 506.1140430189015\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Testing set R2 score: {r2_score(y_test, ridge_test_preds)}\")\n",
                "print(f\"Mean Absolute Error on testing set: {mean_absolute_error(y_test, ridge_test_preds)}\")\n",
                "print(f\"Root Mean Squared Error on testing set: {np.sqrt(mean_squared_error(y_test, ridge_test_preds))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### One more time with tuning hyperparameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=1.86714e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.41347e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=3.42171e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=2.28382e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n",
                        "/home/vscode/.local/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:204: LinAlgWarning: Ill-conditioned matrix (rcond=4.45012e-17): result may not be accurate.\n",
                        "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<style>#sk-container-id-3 {\n",
                            "  /* Definition of color scheme common for light and dark mode */\n",
                            "  --sklearn-color-text: black;\n",
                            "  --sklearn-color-line: gray;\n",
                            "  /* Definition of color scheme for unfitted estimators */\n",
                            "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
                            "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
                            "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
                            "  --sklearn-color-unfitted-level-3: chocolate;\n",
                            "  /* Definition of color scheme for fitted estimators */\n",
                            "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
                            "  --sklearn-color-fitted-level-1: #d4ebff;\n",
                            "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
                            "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
                            "\n",
                            "  /* Specific color for light theme */\n",
                            "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
                            "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
                            "  --sklearn-color-icon: #696969;\n",
                            "\n",
                            "  @media (prefers-color-scheme: dark) {\n",
                            "    /* Redefinition of color scheme for dark theme */\n",
                            "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
                            "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
                            "    --sklearn-color-icon: #878787;\n",
                            "  }\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 pre {\n",
                            "  padding: 0;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 input.sk-hidden--visually {\n",
                            "  border: 0;\n",
                            "  clip: rect(1px 1px 1px 1px);\n",
                            "  clip: rect(1px, 1px, 1px, 1px);\n",
                            "  height: 1px;\n",
                            "  margin: -1px;\n",
                            "  overflow: hidden;\n",
                            "  padding: 0;\n",
                            "  position: absolute;\n",
                            "  width: 1px;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-dashed-wrapped {\n",
                            "  border: 1px dashed var(--sklearn-color-line);\n",
                            "  margin: 0 0.4em 0.5em 0.4em;\n",
                            "  box-sizing: border-box;\n",
                            "  padding-bottom: 0.4em;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-container {\n",
                            "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
                            "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
                            "     so we also need the `!important` here to be able to override the\n",
                            "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
                            "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
                            "  display: inline-block !important;\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-text-repr-fallback {\n",
                            "  display: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-parallel-item,\n",
                            "div.sk-serial,\n",
                            "div.sk-item {\n",
                            "  /* draw centered vertical line to link estimators */\n",
                            "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
                            "  background-size: 2px 100%;\n",
                            "  background-repeat: no-repeat;\n",
                            "  background-position: center center;\n",
                            "}\n",
                            "\n",
                            "/* Parallel-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel-item::after {\n",
                            "  content: \"\";\n",
                            "  width: 100%;\n",
                            "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
                            "  flex-grow: 1;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel {\n",
                            "  display: flex;\n",
                            "  align-items: stretch;\n",
                            "  justify-content: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  position: relative;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel-item {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
                            "  align-self: flex-end;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
                            "  align-self: flex-start;\n",
                            "  width: 50%;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
                            "  width: 0;\n",
                            "}\n",
                            "\n",
                            "/* Serial-specific style estimator block */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-serial {\n",
                            "  display: flex;\n",
                            "  flex-direction: column;\n",
                            "  align-items: center;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  padding-right: 1em;\n",
                            "  padding-left: 1em;\n",
                            "}\n",
                            "\n",
                            "\n",
                            "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
                            "clickable and can be expanded/collapsed.\n",
                            "- Pipeline and ColumnTransformer use this feature and define the default style\n",
                            "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
                            "*/\n",
                            "\n",
                            "/* Pipeline and ColumnTransformer style (default) */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-toggleable {\n",
                            "  /* Default theme specific background. It is overwritten whether we have a\n",
                            "  specific estimator or a Pipeline/ColumnTransformer */\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable label */\n",
                            "#sk-container-id-3 label.sk-toggleable__label {\n",
                            "  cursor: pointer;\n",
                            "  display: block;\n",
                            "  width: 100%;\n",
                            "  margin-bottom: 0;\n",
                            "  padding: 0.5em;\n",
                            "  box-sizing: border-box;\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
                            "  /* Arrow on the left of the label */\n",
                            "  content: \"▸\";\n",
                            "  float: left;\n",
                            "  margin-right: 0.25em;\n",
                            "  color: var(--sklearn-color-icon);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "}\n",
                            "\n",
                            "/* Toggleable content - dropdown */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-toggleable__content {\n",
                            "  max-height: 0;\n",
                            "  max-width: 0;\n",
                            "  overflow: hidden;\n",
                            "  text-align: left;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-toggleable__content pre {\n",
                            "  margin: 0.2em;\n",
                            "  border-radius: 0.25em;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
                            "  /* Expand drop-down */\n",
                            "  max-height: 200px;\n",
                            "  max-width: 100%;\n",
                            "  overflow: auto;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
                            "  content: \"▾\";\n",
                            "}\n",
                            "\n",
                            "/* Pipeline/ColumnTransformer-specific style */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific style */\n",
                            "\n",
                            "/* Colorize estimator box */\n",
                            "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
                            "#sk-container-id-3 div.sk-label label {\n",
                            "  /* The background is the default theme color */\n",
                            "  color: var(--sklearn-color-text-on-default-background);\n",
                            "}\n",
                            "\n",
                            "/* On hover, darken the color of the background */\n",
                            "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Label box, darken color on hover, fitted */\n",
                            "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Estimator label */\n",
                            "\n",
                            "#sk-container-id-3 div.sk-label label {\n",
                            "  font-family: monospace;\n",
                            "  font-weight: bold;\n",
                            "  display: inline-block;\n",
                            "  line-height: 1.2em;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-label-container {\n",
                            "  text-align: center;\n",
                            "}\n",
                            "\n",
                            "/* Estimator-specific */\n",
                            "#sk-container-id-3 div.sk-estimator {\n",
                            "  font-family: monospace;\n",
                            "  border: 1px dotted var(--sklearn-color-border-box);\n",
                            "  border-radius: 0.25em;\n",
                            "  box-sizing: border-box;\n",
                            "  margin-bottom: 0.5em;\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-0);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-estimator.fitted {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-0);\n",
                            "}\n",
                            "\n",
                            "/* on hover */\n",
                            "#sk-container-id-3 div.sk-estimator:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-2);\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-2);\n",
                            "}\n",
                            "\n",
                            "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
                            "\n",
                            "/* Common style for \"i\" and \"?\" */\n",
                            "\n",
                            ".sk-estimator-doc-link,\n",
                            "a:link.sk-estimator-doc-link,\n",
                            "a:visited.sk-estimator-doc-link {\n",
                            "  float: right;\n",
                            "  font-size: smaller;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1em;\n",
                            "  height: 1em;\n",
                            "  width: 1em;\n",
                            "  text-decoration: none !important;\n",
                            "  margin-left: 1ex;\n",
                            "  /* unfitted */\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted,\n",
                            "a:link.sk-estimator-doc-link.fitted,\n",
                            "a:visited.sk-estimator-doc-link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
                            ".sk-estimator-doc-link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover,\n",
                            "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
                            ".sk-estimator-doc-link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "/* Span, style for the box shown on hovering the info icon */\n",
                            ".sk-estimator-doc-link span {\n",
                            "  display: none;\n",
                            "  z-index: 9999;\n",
                            "  position: relative;\n",
                            "  font-weight: normal;\n",
                            "  right: .2ex;\n",
                            "  padding: .5ex;\n",
                            "  margin: .5ex;\n",
                            "  width: min-content;\n",
                            "  min-width: 20ex;\n",
                            "  max-width: 50ex;\n",
                            "  color: var(--sklearn-color-text);\n",
                            "  box-shadow: 2pt 2pt 4pt #999;\n",
                            "  /* unfitted */\n",
                            "  background: var(--sklearn-color-unfitted-level-0);\n",
                            "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link.fitted span {\n",
                            "  /* fitted */\n",
                            "  background: var(--sklearn-color-fitted-level-0);\n",
                            "  border: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "\n",
                            ".sk-estimator-doc-link:hover span {\n",
                            "  display: block;\n",
                            "}\n",
                            "\n",
                            "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
                            "\n",
                            "#sk-container-id-3 a.estimator_doc_link {\n",
                            "  float: right;\n",
                            "  font-size: 1rem;\n",
                            "  line-height: 1em;\n",
                            "  font-family: monospace;\n",
                            "  background-color: var(--sklearn-color-background);\n",
                            "  border-radius: 1rem;\n",
                            "  height: 1rem;\n",
                            "  width: 1rem;\n",
                            "  text-decoration: none;\n",
                            "  /* unfitted */\n",
                            "  color: var(--sklearn-color-unfitted-level-1);\n",
                            "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
                            "  /* fitted */\n",
                            "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
                            "  color: var(--sklearn-color-fitted-level-1);\n",
                            "}\n",
                            "\n",
                            "/* On hover */\n",
                            "#sk-container-id-3 a.estimator_doc_link:hover {\n",
                            "  /* unfitted */\n",
                            "  background-color: var(--sklearn-color-unfitted-level-3);\n",
                            "  color: var(--sklearn-color-background);\n",
                            "  text-decoration: none;\n",
                            "}\n",
                            "\n",
                            "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
                            "  /* fitted */\n",
                            "  background-color: var(--sklearn-color-fitted-level-3);\n",
                            "}\n",
                            "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Ridge(),\n",
                            "             param_grid={&#x27;alpha&#x27;: [0, 0.1, 1, 10, 100],\n",
                            "                         &#x27;max_iter&#x27;: [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=Ridge(),\n",
                            "             param_grid={&#x27;alpha&#x27;: [0, 0.1, 1, 10, 100],\n",
                            "                         &#x27;max_iter&#x27;: [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring=&#x27;r2&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: Ridge</label><div class=\"sk-toggleable__content fitted\"><pre>Ridge()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Ridge<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.Ridge.html\">?<span>Documentation for Ridge</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>Ridge()</pre></div> </div></div></div></div></div></div></div></div></div>"
                        ],
                        "text/plain": [
                            "GridSearchCV(estimator=Ridge(),\n",
                            "             param_grid={'alpha': [0, 0.1, 1, 10, 100],\n",
                            "                         'max_iter': [50, 100, 500, 1000, 2000, 5000]},\n",
                            "             scoring='r2')"
                        ]
                    },
                    "execution_count": 82,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid_ridge_model = GridSearchCV(Ridge(), param_grid=hyperparameters, scoring='r2')\n",
                "\n",
                "grid_ridge_model.fit(X_train, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'alpha': 100, 'max_iter': 50}"
                        ]
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "grid_ridge_model.best_params_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Quick visuals to compare models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>mean_fit_time</th>\n",
                            "      <th>std_fit_time</th>\n",
                            "      <th>mean_score_time</th>\n",
                            "      <th>std_score_time</th>\n",
                            "      <th>param_alpha</th>\n",
                            "      <th>param_max_iter</th>\n",
                            "      <th>params</th>\n",
                            "      <th>split0_test_score</th>\n",
                            "      <th>split1_test_score</th>\n",
                            "      <th>split2_test_score</th>\n",
                            "      <th>split3_test_score</th>\n",
                            "      <th>split4_test_score</th>\n",
                            "      <th>mean_test_score</th>\n",
                            "      <th>std_test_score</th>\n",
                            "      <th>rank_test_score</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0.016716</td>\n",
                            "      <td>0.003255</td>\n",
                            "      <td>0.003604</td>\n",
                            "      <td>0.001054</td>\n",
                            "      <td>0</td>\n",
                            "      <td>50</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 50}</td>\n",
                            "      <td>-944.048785</td>\n",
                            "      <td>-1190.781172</td>\n",
                            "      <td>-861.377021</td>\n",
                            "      <td>-721.591290</td>\n",
                            "      <td>-1088.814231</td>\n",
                            "      <td>-961.322500</td>\n",
                            "      <td>165.326318</td>\n",
                            "      <td>27</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0.025402</td>\n",
                            "      <td>0.003186</td>\n",
                            "      <td>0.004610</td>\n",
                            "      <td>0.001738</td>\n",
                            "      <td>0</td>\n",
                            "      <td>100</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 100}</td>\n",
                            "      <td>-884.609571</td>\n",
                            "      <td>-1127.852045</td>\n",
                            "      <td>-719.344174</td>\n",
                            "      <td>-725.112770</td>\n",
                            "      <td>-1004.698116</td>\n",
                            "      <td>-892.323335</td>\n",
                            "      <td>158.771868</td>\n",
                            "      <td>9</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0.106115</td>\n",
                            "      <td>0.013860</td>\n",
                            "      <td>0.004116</td>\n",
                            "      <td>0.002126</td>\n",
                            "      <td>0</td>\n",
                            "      <td>500</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 500}</td>\n",
                            "      <td>-695.238257</td>\n",
                            "      <td>-1527.124691</td>\n",
                            "      <td>-717.314846</td>\n",
                            "      <td>-741.334378</td>\n",
                            "      <td>-939.379079</td>\n",
                            "      <td>-924.078250</td>\n",
                            "      <td>313.819469</td>\n",
                            "      <td>12</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0.176276</td>\n",
                            "      <td>0.016887</td>\n",
                            "      <td>0.002686</td>\n",
                            "      <td>0.000457</td>\n",
                            "      <td>0</td>\n",
                            "      <td>1000</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 1000}</td>\n",
                            "      <td>-628.552277</td>\n",
                            "      <td>-1738.538236</td>\n",
                            "      <td>-741.009217</td>\n",
                            "      <td>-701.087810</td>\n",
                            "      <td>-896.819910</td>\n",
                            "      <td>-941.201490</td>\n",
                            "      <td>408.214813</td>\n",
                            "      <td>21</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0.370948</td>\n",
                            "      <td>0.058184</td>\n",
                            "      <td>0.005061</td>\n",
                            "      <td>0.002122</td>\n",
                            "      <td>0</td>\n",
                            "      <td>2000</td>\n",
                            "      <td>{'alpha': 0, 'max_iter': 2000}</td>\n",
                            "      <td>-626.765550</td>\n",
                            "      <td>-1852.596443</td>\n",
                            "      <td>-710.905216</td>\n",
                            "      <td>-650.665718</td>\n",
                            "      <td>-801.637728</td>\n",
                            "      <td>-928.514131</td>\n",
                            "      <td>465.968095</td>\n",
                            "      <td>15</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
                            "0       0.016716      0.003255         0.003604        0.001054           0   \n",
                            "1       0.025402      0.003186         0.004610        0.001738           0   \n",
                            "2       0.106115      0.013860         0.004116        0.002126           0   \n",
                            "3       0.176276      0.016887         0.002686        0.000457           0   \n",
                            "4       0.370948      0.058184         0.005061        0.002122           0   \n",
                            "\n",
                            "  param_max_iter                          params  split0_test_score  \\\n",
                            "0             50    {'alpha': 0, 'max_iter': 50}        -944.048785   \n",
                            "1            100   {'alpha': 0, 'max_iter': 100}        -884.609571   \n",
                            "2            500   {'alpha': 0, 'max_iter': 500}        -695.238257   \n",
                            "3           1000  {'alpha': 0, 'max_iter': 1000}        -628.552277   \n",
                            "4           2000  {'alpha': 0, 'max_iter': 2000}        -626.765550   \n",
                            "\n",
                            "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
                            "0       -1190.781172        -861.377021        -721.591290       -1088.814231   \n",
                            "1       -1127.852045        -719.344174        -725.112770       -1004.698116   \n",
                            "2       -1527.124691        -717.314846        -741.334378        -939.379079   \n",
                            "3       -1738.538236        -741.009217        -701.087810        -896.819910   \n",
                            "4       -1852.596443        -710.905216        -650.665718        -801.637728   \n",
                            "\n",
                            "   mean_test_score  std_test_score  rank_test_score  \n",
                            "0      -961.322500      165.326318               27  \n",
                            "1      -892.323335      158.771868                9  \n",
                            "2      -924.078250      313.819469               12  \n",
                            "3      -941.201490      408.214813               21  \n",
                            "4      -928.514131      465.968095               15  "
                        ]
                    },
                    "execution_count": 75,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "lasso_grid_results_df = pd.DataFrame(grid_model.cv_results_)\n",
                "\n",
                "lasso_grid_results_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<Axes: xlabel='param_alpha', ylabel='mean_test_score'>"
                        ]
                    },
                    "execution_count": 81,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "sns.scatterplot(data=lasso_grid_results_df, x = 'param_alpha', y = 'mean_test_score')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 64-bit ('3.8.13')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
